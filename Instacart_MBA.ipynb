{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c99a2918-5cad-4408-add1-128c74b971dd",
   "metadata": {},
   "source": [
    "# Decoding Consumer Choices:\n",
    "### An In-Depth Analysis of Instacart Market Baskets\n",
    "\n",
    "### IDC 6940 Capstone Project - Fall 2025 \n",
    "\n",
    "**Team:** Darien Diaz, Nancy Lopez  \n",
    "**Mentor:** Agoritsa Polyzou  \n",
    "**Due Date:** December 8th, 2025  \n",
    "\n",
    "#### Data:\n",
    "- Data set from Kaggle: https://www.kaggle.com/datasets/yasserh/instacart-online-grocery-basket-analysis-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6fa6bc-5ed2-443d-9303-94534ece94f9",
   "metadata": {},
   "source": [
    "#### Install and Import required librarries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d2e041-164b-44e9-b529-875f17df8c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (commented out after first run)\n",
    "#pip install efficient-apriori\n",
    "#pip install mlxtend\n",
    "#pip install prefixspan\n",
    "#pip install networkx\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756bf77c-3cd8-4850-8504-44233a51066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import itertools\n",
    "import re\n",
    "import nltk\n",
    "import scipy\n",
    "import textwrap\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from efficient_apriori import apriori\n",
    "from typing import List, Union\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import PCA\n",
    "from wordcloud import WordCloud\n",
    "from prefixspan import PrefixSpan\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d00b54-0abd-4bf2-a990-9f99e553661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all characters in columns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# display all rows\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a70600-e176-4412-acf8-3a1936877433",
   "metadata": {},
   "source": [
    "#### Data Load and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b1efa-86ec-4a11-9fd9-59dadecdb027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Load csv files\n",
    "# -------------------------------------------------\n",
    "def load_csv_from_zip(zip_file, dtype_dict=None, chunksize=None):\n",
    "    \"\"\"Loads a CSV from a ZIP file, selecting the correct file while ignoring macOS metadata.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as z:\n",
    "        # Filter out macOS metadata and ensure we have exactly one CSV\n",
    "        filenames = [f for f in z.namelist() if f.endswith('.csv') and not f.startswith('__MACOSX') and not f.startswith('._')]\n",
    "        if len(filenames) != 1:\n",
    "            raise ValueError(f\"Expected exactly 1 CSV file, but found {len(filenames)}: {filenames}\")\n",
    "\n",
    "        filename = filenames[0]\n",
    "        with z.open(filename) as f:\n",
    "            if chunksize:\n",
    "                return pd.concat(pd.read_csv(f, dtype=dtype_dict, chunksize=chunksize))\n",
    "            else:\n",
    "                return pd.read_csv(f, dtype=dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f2a62-9e93-44da-895a-ec21d594cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Define dtypes for efficiency\n",
    "# -------------------------------------------------\n",
    "dtypes_orders = {\n",
    "    'order_id': 'int32', 'order_number': 'int32', 'user_id': 'int32',\n",
    "    'order_dow': 'int8', 'order_hour_of_day': 'int8', 'days_since_prior_order': 'float32'\n",
    "}\n",
    "dtypes_order_products = {\n",
    "    'order_id': 'int32', 'product_id': 'int32',\n",
    "    'add_to_cart_order': 'int16', 'reordered': 'int8'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02897882-1400-4d34-99fe-af489a0c4bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Load datasets from zip files\n",
    "# -------------------------------------------------\n",
    "orders = load_csv_from_zip('orders.csv.zip', dtype_dict=dtypes_orders)\n",
    "order_products_prior = load_csv_from_zip('order_products__prior.csv.zip', dtype_dict=dtypes_order_products)\n",
    "order_products_train = load_csv_from_zip('order_products__train.csv.zip', dtype_dict=dtypes_order_products)\n",
    "products = load_csv_from_zip('products.csv.zip', dtype_dict=dtypes_order_products)\n",
    "departments = load_csv_from_zip('departments.csv.zip')\n",
    "aisles = load_csv_from_zip('aisles.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2e9a0-6586-4b7e-9275-380707b4620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Combine prior and train datasets\n",
    "# -------------------------------------------------\n",
    "order_products_all = pd.concat([order_products_prior, order_products_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb25447d-5609-4da0-89d9-ac891d12c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Merge with orders information\n",
    "# -------------------------------------------------\n",
    "order_all_dtl = pd.merge(order_products_all, orders, on='order_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f73663c-5d35-411f-a7c4-4ce1a3f9a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Merge with products information\n",
    "# -------------------------------------------------\n",
    "orders_all = pd.merge(order_all_dtl, products, on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38c156-dd02-449a-8ed6-f6d2315e3666",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis - EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce5a51-b01b-43ec-bd0d-03e8e44c3854",
   "metadata": {},
   "source": [
    "### 1.1. Summary Statistics for the Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5b4fa-335e-488d-aa75-7f8a172cee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Stats for the whole dataset\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Number of orders per user\n",
    "orders_per_user = orders_all.groupby('user_id')['order_id'].nunique()\n",
    "\n",
    "# Number of unique products per user\n",
    "products_per_user = orders_all.groupby('user_id')['product_id'].nunique()\n",
    "\n",
    "# Number of products per order (basket size)\n",
    "products_per_order = orders_all.groupby('order_id')['product_id'].nunique()\n",
    "\n",
    "# Reorder rate\n",
    "reorder_rate = orders_all['reordered'].mean()\n",
    "\n",
    "# Days Since Prior Order\n",
    "days_since_prior_order = orders_all.groupby('order_id')['days_since_prior_order'].mean()\n",
    "\n",
    "# Summarize\n",
    "stats_summary = pd.DataFrame({\n",
    "    'avg_orders_per_user': [orders_per_user.mean()],\n",
    "    'avg_products_per_user': [products_per_user.mean()],\n",
    "    'avg_products_per_order': [products_per_order.mean()],\n",
    "    'reorder_rate': [reorder_rate],\n",
    "    'total_orders': [orders_all['order_id'].nunique()],\n",
    "    'min_orders_per_user': [orders_per_user.min()],\n",
    "    'max_orders_per_user': [orders_per_user.max()],\n",
    "    'total_products': [orders_all['product_id'].nunique()],\n",
    "    'total_users': [orders_all['user_id'].nunique()]\n",
    "})\n",
    "\n",
    "print(\"Summary Statistics for Complete Dataset :\")\n",
    "display(stats_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e00ee9-fe04-42da-9aca-9eb71be61f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Boxplots for the whole dataset\n",
    "# -------------------------------------------------\n",
    "\n",
    "# remove grid lines\n",
    "sns.set(style=\"white\", palette=\"tab10\", font_scale=1.1)\n",
    "\n",
    "# Custom color palette\n",
    "custom_colors = sns.color_palette(\"tab10\", 4)\n",
    "\n",
    "# Create figure with 4 subplots\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 6), dpi=120)\n",
    "\n",
    "# Add title\n",
    "fig.suptitle('Distribution of Key Customer and Order Metrics (Whole Dataset)', fontsize=18, weight='bold')\n",
    "\n",
    "# Boxplot 1: Orders per User\n",
    "sns.boxplot(y=orders_per_user, ax=axes[0], color=custom_colors[0], width=0.5, showfliers=True)\n",
    "axes[0].set_title('Orders per User', fontsize=14)\n",
    "axes[0].set_ylabel('Number of Orders', fontsize=20, labelpad=10)\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].tick_params(axis='y', labelsize=18)\n",
    "axes[0].spines[['top', 'right']].set_visible(False)\n",
    "axes[0].grid(False)\n",
    "\n",
    "# Boxplot 2: Unique Products per User\n",
    "sns.boxplot(y=products_per_user, ax=axes[1], color=custom_colors[1], width=0.5, showfliers=True)\n",
    "axes[1].set_title('Unique Products per User', fontsize=14)\n",
    "axes[1].set_ylabel('Number of Unique Products', fontsize=20, labelpad=10)\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].tick_params(axis='y', labelsize=18)\n",
    "axes[1].spines[['top', 'right']].set_visible(False)\n",
    "axes[1].grid(False)\n",
    "\n",
    "# Boxplot 3: Products per Order (Basket Size)\n",
    "sns.boxplot(y=products_per_order, ax=axes[2], color=custom_colors[2], width=0.5, showfliers=True)\n",
    "axes[2].set_title('Products per Order (Basket Size)', fontsize=14)\n",
    "axes[2].set_ylabel('Number of Products', fontsize=20, labelpad=10)\n",
    "axes[2].set_xlabel('')\n",
    "axes[2].tick_params(axis='y', labelsize=18)\n",
    "axes[2].spines[['top', 'right']].set_visible(False)\n",
    "axes[2].grid(False)\n",
    "\n",
    "# Boxplot 4: Days Since Prior Order\n",
    "sns.boxplot(y=days_since_prior_order, ax=axes[3], color=custom_colors[3], width=0.5, showfliers=True)\n",
    "axes[3].set_title('Days Since Prior Order', fontsize=14)\n",
    "axes[3].set_ylabel('Number of Days', fontsize=20, labelpad=10)\n",
    "axes[3].set_xlabel('')\n",
    "axes[3].tick_params(axis='y', labelsize=18)\n",
    "axes[3].spines[['top', 'right']].set_visible(False)\n",
    "axes[3].grid(False)\n",
    "\n",
    "# Layout adjustments\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0264c22-ada9-41b6-8bd6-841bf4aadacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Stats for the whole dataset\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Create a dictionary of datasets\n",
    "datasets = {\n",
    "    'Orders per User': orders_per_user,\n",
    "    'Products per User': products_per_user, \n",
    "    'Products per Order': products_per_order, \n",
    "    'Days since Prior order': days_since_prior_order\n",
    "}    \n",
    "    \n",
    "# Generate describe() for each and store results\n",
    "summary_dict = {}\n",
    "for name, df in datasets.items():\n",
    "    desc = df.describe()\n",
    "    summary_dict[name] = desc\n",
    "\n",
    "# Concatenate along columns with hierarchical index\n",
    "summary_df = pd.concat(summary_dict, axis=1)\n",
    "formatted_summary = summary_df.applymap(lambda x: f\"{x:,.2f}\")\n",
    "\n",
    "# View metrics as rows and datasets as columns\n",
    "formatted_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da854a7-706c-46c4-b524-f527a7060aba",
   "metadata": {},
   "source": [
    "### 1.2. Behavior Patterns - When do customers purchase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb046b0-a6f4-4736-8c6b-98f15d25057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by order_id to get unique orders and extract first occurrence of day and hour\n",
    "order_times = orders_all.groupby('order_id')[['order_dow', 'order_hour_of_day']].first()\n",
    "\n",
    "# Create a pivot table: rows = day of week, columns = hour of day, values = order counts\n",
    "dow_hour_matrix = (\n",
    "    order_times\n",
    "    .groupby(['order_dow', 'order_hour_of_day'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# Reorder day of week for better readability (0=Sunday, 6=Saturday)\n",
    "dow_labels = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "dow_hour_matrix.index = [dow_labels[d] for d in dow_hour_matrix.index]\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(\n",
    "    dow_hour_matrix,\n",
    "    cmap='YlGnBu',\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "\n",
    "# Increase all font sizes\n",
    "plt.title('Heatmap of Orders by Day of Week and Hour of Day', fontsize=18, pad=15)\n",
    "plt.xlabel('Hour of Day (0 = Midnight)', fontsize=16, labelpad=10)\n",
    "plt.ylabel('Day of Week', fontsize=16, labelpad=10)\n",
    "\n",
    "# tick label sizes\n",
    "plt.xticks(fontsize=16, rotation=0)\n",
    "plt.yticks(fontsize=16, rotation=0)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe90ab1c-8580-46db-9aad-93d70a0f99b2",
   "metadata": {},
   "source": [
    "### 1.3. Weekend vs Weekday Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5777442-389d-4f22-8a33-c16d28c574c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are certain product categories more popular on weekends vs. weekdays?\n",
    "# Prepare data\n",
    "orders_dept = pd.merge(orders_all, departments, on='department_id', how='left')\n",
    "orders_dept['is_weekend'] = orders_dept['order_dow'].apply(lambda x: 'Weekend' if x in [0, 6] else 'Weekday')\n",
    "\n",
    "# Count items sold per department per day type\n",
    "dept_day_counts = orders_dept.groupby(['department', 'is_weekend'])['order_id'].count().unstack(fill_value=0)\n",
    "\n",
    "# Normalize by total items sold in each category\n",
    "dept_day_pct = dept_day_counts.apply(lambda x: x / x.sum(), axis=0)\n",
    "\n",
    "# Compute % difference\n",
    "dept_day_pct['Weekend_vs_Weekday_Diff'] = dept_day_pct['Weekend'] - dept_day_pct['Weekday']\n",
    "dept_day_pct_sorted = dept_day_pct.sort_values('Weekend_vs_Weekday_Diff', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set_style(\"white\")  # Clean white background, no grid\n",
    "barplot = sns.barplot(\n",
    "    y=dept_day_pct_sorted.index,\n",
    "    x=dept_day_pct_sorted['Weekend_vs_Weekday_Diff'] * 100,\n",
    "    palette='coolwarm'\n",
    ")\n",
    "\n",
    "# Titles and labels\n",
    "#plt.title('Percentage Point Difference in Items Sold: Weekend vs. Weekday', fontsize=14, color='black')\n",
    "plt.xlabel('Weekend - Weekday (% of total items sold)', fontsize=16, color='black')\n",
    "plt.ylabel('Department', fontsize=16, color='black')\n",
    "plt.axvline(0, color='black', linestyle='--')\n",
    "\n",
    "# Set all text to black\n",
    "barplot.tick_params(colors='black')\n",
    "plt.xticks(color='black', fontsize=16)\n",
    "plt.yticks(color='black', fontsize=16)\n",
    "\n",
    "# Add custom legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor=sns.color_palette(\"coolwarm\")[0], label='Popular on Weekends'),\n",
    "    Patch(facecolor=sns.color_palette(\"coolwarm\")[-1], label='Popular on Weekdays')\n",
    "]\n",
    "plt.legend(handles=legend_elements, \n",
    "           loc='lower right', frameon=False, fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b76c5-9cd5-491e-b1d9-2e9086eab347",
   "metadata": {},
   "source": [
    "### 1.4. New vs Loyal Customer Reorder Rate Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddda0ba-d8a4-4fff-b126-91232524070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do new users (fewer orders) have different buying patterns compared to loyal customers?\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Analysis: New vs. Loyal Customers\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Calculate max order number (total orders) for each user\n",
    "user_total_orders = orders.groupby('user_id')['order_number'].max().reset_index(name='total_orders')\n",
    "\n",
    "# Define Loyalty Segments based on total orders\n",
    "def categorize_loyalty(orders):\n",
    "    if orders  <= 5:\n",
    "        return 'New' # Customer (<= 5 Orders)'\n",
    "    elif orders <= 15:\n",
    "        return 'Regular' # Customer (6-15 Orders)'\n",
    "    else:\n",
    "        return 'Loyal' # Customer (16+ Orders)'\n",
    "\n",
    "\n",
    "user_total_orders['loyalty_group'] = user_total_orders['total_orders'].apply(categorize_loyalty)\n",
    "\n",
    "# Merge loyalty group back into the transaction data (orders_all)\n",
    "loyalty_df = pd.merge(orders_all, user_total_orders[['user_id', 'loyalty_group']], on='user_id', how='left')\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Aggregate Metrics by Loyalty Group\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Calculate reorder rate and average basket size by loyalty group\n",
    "loyalty_stats = loyalty_df.groupby('loyalty_group').agg(\n",
    "    avg_reorder_rate=('reordered', 'mean'),\n",
    "    avg_basket_size=('order_id', lambda x: loyalty_df.loc[x.index].groupby('order_id')['product_id'].nunique().mean())\n",
    ").reset_index()\n",
    "\n",
    "# Reorder the loyalty groups for visualization\n",
    "loyalty_order = ['New', 'Regular', 'Loyal']\n",
    "loyalty_stats['loyalty_group'] = pd.Categorical(loyalty_stats['loyalty_group'], categories=loyalty_order, ordered=True)\n",
    "loyalty_stats = loyalty_stats.sort_values('loyalty_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce0b3f4-aad2-4a17-8b5c-eca40a1ac2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Visualization\n",
    "# -------------------------------------------------\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.suptitle('Buying Patterns by Customer Loyalty Group', fontsize=24, fontweight='bold')\n",
    "\n",
    "# Plot: Average Reorder Rate\n",
    "sns.barplot(\n",
    "    x='loyalty_group',\n",
    "    y='avg_reorder_rate',\n",
    "    data=loyalty_stats,\n",
    "    ax=ax,\n",
    "    palette='viridis'\n",
    ")\n",
    "\n",
    "# Titles, labels, and ticks\n",
    "ax.set_title('Average Reorder Rate', fontsize=14, pad=15)\n",
    "ax.set_ylabel('Reorder Rate', fontsize=14, labelpad=10)\n",
    "ax.set_xlabel('Customer Loyalty Group', fontsize=14, labelpad=10)\n",
    "ax.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "# Clean visual style\n",
    "ax.spines[['top', 'right']].set_visible(False)\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c9c0d-5a20-406e-adf3-e346c232fb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for one specific user and show only selected columns (example)\n",
    "user1_orders = orders_all.loc[orders_all['user_id'] == 206206, \n",
    "                              ['user_id', 'order_id', 'days_since_prior_order']]\n",
    "\n",
    "#print(user1_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810461b3-d2b2-4c5f-a456-8d87b955ecec",
   "metadata": {},
   "source": [
    "### 1.5. Purchasing Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f93047-c374-47be-84e0-fd2d5c44b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Average Days Since Prior Order per User\n",
    "# -------------------------------------------------\n",
    "\n",
    "# 1. Clean column names\n",
    "orders_all.columns = orders_all.columns.str.strip().str.lower()\n",
    "\n",
    "# 2. Remove rows with missing values\n",
    "plot_data = orders_all.dropna(subset=['days_since_prior_order'])\n",
    "\n",
    "# 3. Compute the average per user\n",
    "avg_days_per_user = (\n",
    "    plot_data.groupby('user_id', as_index=False)['days_since_prior_order']\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# 4. Round and count how many users fall into each average-day value\n",
    "avg_days_per_user['avg_days'] = avg_days_per_user['days_since_prior_order'].round()\n",
    "\n",
    "avg_days_counts = avg_days_per_user['avg_days'].value_counts().reset_index()\n",
    "\n",
    "# print columns\n",
    "print(\"Columns after reset_index:\", avg_days_counts.columns.tolist())\n",
    "\n",
    "# 5. Rename columns based on actual names returned\n",
    "cols = list(avg_days_counts.columns)\n",
    "avg_days_counts.columns = ['avg_days', 'user_count'] if len(cols) == 2 else cols\n",
    "\n",
    "# 6. Sort by avg_days\n",
    "avg_days_counts['avg_days'] = avg_days_counts['avg_days'].astype(int)\n",
    "\n",
    "# 7. Plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(\n",
    "    data=avg_days_counts,\n",
    "    x='avg_days',\n",
    "    y='user_count',\n",
    "    color='darkblue',\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "plt.title('Average Days Since Prior Order per User', fontsize=22, pad=15)\n",
    "plt.xlabel('Avg Days Since Prior Order', fontsize=20, labelpad=10)\n",
    "plt.ylabel('Number of Users', fontsize=20, labelpad=10)\n",
    "plt.xticks(ticks=avg_days_counts['avg_days'].astype(int).unique(),fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbdf2d7-ef1c-461e-92cc-e6026c8caf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Average Days Since Prior Order (per Order)\n",
    "# -------------------------------------------------\n",
    "\n",
    "# 1. Prepare the data: Filter out NaNs, as they have no prior order time.\n",
    "plot_data = orders.dropna(subset=['days_since_prior_order'])\n",
    " \n",
    "# 2. Drop duplicates to ensure we only count each unique\n",
    "unique_user_days = plot_data[['user_id', 'days_since_prior_order']]\n",
    " \n",
    "# 3. Create the count for the plot\n",
    "# Group by the days_since_prior_order and count the remaining unique rows.\n",
    "unique_user_counts = unique_user_days['days_since_prior_order'].value_counts().reset_index()\n",
    "unique_user_counts.columns = ['days_since_prior_order', 'unique_user_count']\n",
    " \n",
    "# 4. Sort the data by day for correct plotting order\n",
    "unique_user_counts = unique_user_counts.sort_values('days_since_prior_order')\n",
    "\n",
    "# 5. Generate the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    " \n",
    "sns.barplot(\n",
    "    data=unique_user_counts,\n",
    "    x='days_since_prior_order',\n",
    "    y='unique_user_count',\n",
    "    color='darkblue',\n",
    "    edgecolor='black'\n",
    ")\n",
    " \n",
    "# 6. Set labels, title, and grid\n",
    "plt.title('Distribution of Days Since Prior Order')\n",
    "plt.xlabel('Days Since Prior Order')\n",
    "plt.ylabel('Count of Unique Orders')\n",
    " \n",
    "# Customize x-axis ticks for better readability\n",
    "plt.xticks(range(0, 31, 5))\n",
    "plt.grid(axis='y')\n",
    "plt.grid(False)\n",
    "\n",
    "# 7. Display the plot\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24452a45-d869-47d3-bb4a-7befa16789b1",
   "metadata": {},
   "source": [
    "### 1.6. Top Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b5dcce-5b21-46be-b08f-4ab52df494bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate the total orders and total reorders per product\n",
    "product_reorder_stats = orders_all.groupby('product_id').agg(\n",
    "    total_orders=('order_id', 'count'),\n",
    "    total_reorders=('reordered', 'sum')\n",
    ")\n",
    "\n",
    "# 2. Calculate the reorder rate\n",
    "product_reorder_stats['reorder_rate'] = product_reorder_stats['total_reorders'] / product_reorder_stats['total_orders']\n",
    "\n",
    "# 3. Filter for products ordered a minimum number of times (e.g., at least 40)\n",
    "MIN_ORDERS = 40\n",
    "frequent_products = product_reorder_stats[product_reorder_stats['total_orders'] >= MIN_ORDERS]\n",
    "\n",
    "# Merge with product names for readability \n",
    "if 'products' in globals():\n",
    "    frequent_products = pd.merge(\n",
    "        frequent_products, \n",
    "        products[['product_id', 'product_name']], \n",
    "        on='product_id', \n",
    "        how='left'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f683cbe-cfe4-4119-a346-b74174dce20b",
   "metadata": {},
   "source": [
    "#### 1.6.1. Top Products Based on Reorder Rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a453492-105d-41d6-85f0-fce727075d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sort and display the top n based on reorder rate\n",
    "top_reorder_products = frequent_products.sort_values('reorder_rate', ascending=False).head(1000)\n",
    "\n",
    "top_reorder_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f4103-089c-436c-9662-df5d20269916",
   "metadata": {},
   "source": [
    "#### 1.6.2. Top Products Based on Total Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271bd80-dd90-4d6a-88f6-cef66492233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sort and display the top n based on total orders\n",
    "top_order_products = frequent_products.sort_values('total_orders', ascending=False).head(1000)\n",
    "\n",
    "top_order_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aebf9a1-e1df-44e4-8123-d6203283d670",
   "metadata": {},
   "source": [
    "## 2. Apply NLP to Group Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e287ae0-1b70-485d-9b6d-56b63d96ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud setup\n",
    "stop_words = set(ENGLISH_STOP_WORDS)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# stopwords with grocery-specific noise\n",
    "custom_stopwords = {\n",
    "    \"oz\", \"pack\", \"ct\", \"count\", \"pk\", \"fl\", \"lb\", \"lbs\", \"bottle\", \"bottles\", \"gallon\", \"gallons\", \"bag\", \"organic\", \"hass\", \n",
    "    \"large\", \"small\", \"red\", \"green\", \"white\", \"yellow\", \"fresh\", \"bunch\", \"chunk\", \"seedless\", \"cut\", \"quick\", \"easy\", \"sliced\"\n",
    "}\n",
    "all_stopwords = stop_words.union(custom_stopwords)\n",
    "\n",
    "def clean_product_name(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    # remove numbers and size patterns\n",
    "    text = re.sub(r'\\d+(\\.\\d+)?\\s*(oz|fl|lb|g|kg|ml|l|pack|pk|ct|count)\\b', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    # keep only letters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    # collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # remove stopwords\n",
    "    tokens = [t for t in text.split() if t not in all_stopwords]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ce1af-0eda-48a2-b089-2faa6dab8a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new column\n",
    "frequent_products[\"product_name_clean\"] = frequent_products[\"product_name\"].astype(str).apply(clean_product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2779402-d89f-4962-9794-ae88cbd6817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add department_id to frequent_products\n",
    "frequent_products = frequent_products.merge(\n",
    "    products[['product_id', 'department_id']],\n",
    "    on='product_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3af80-c859-411d-a376-f2f7558cbf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "frequent_products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc17fe1f-a7a5-4a4a-8ac5-28904c6e0038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add department name\n",
    "frequent_products = frequent_products.merge(\n",
    "    departments[['department_id', 'department']],\n",
    "    on='department_id',\n",
    "    how='left'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f76cc-8235-4fcc-8a0f-43a834fb0574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "frequent_products = frequent_products[\n",
    "    [\n",
    "        'product_id', 'product_name', 'product_name_clean',\n",
    "        'department_id', 'department',\n",
    "        'total_orders', 'total_reorders', 'reorder_rate'\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c11e8e-8c5e-4702-bb62-edc936615cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "frequent_products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75650c15-37ab-48ef-ac12-b554627d5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_products.to_csv(\"frequent_products_with_department_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6392bdc-7e23-449f-90a9-92cf8ddcf6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize further\n",
    "\n",
    "# extend list as more patterns discovered\n",
    "def normalize_product_group(row):\n",
    "    name = str(row[\"product_name_clean\"]).lower()\n",
    "    dept = str(row.get(\"department\", \"\")).lower() \n",
    "\n",
    "    # --- MILK & CREAMERS ---\n",
    "    if (\n",
    "        \"milk\" in name or \"oatmilk\" in name or\n",
    "        (\"almond\" in name and \"milk\" in name) or\n",
    "        (\"soy\" in name and \"milk\" in name) or\n",
    "        (\"coconut\" in name and \"milk\" in name) or\n",
    "        \"half half\" in name or\n",
    "        \"half & half\" in name or\n",
    "        (\"creamer\" in name and \"coffee\" in name)\n",
    "    ):\n",
    "        return \"milk\"\n",
    "\n",
    "    # --- BREAD / BAKERY LOAVES ---\n",
    "    if (\n",
    "        \"bread\" in name or\n",
    "        \"baguette\" in name or\n",
    "        \"sourdough\" in name or\n",
    "        \"brioche\" in name or\n",
    "        \"loaf\" in name or\n",
    "        \"sandwich roll\" in name or\n",
    "        (\"roll\" in name and \"dinner\" not in name) or\n",
    "        (\"bun\" in name)\n",
    "    ):\n",
    "        return \"bread\"\n",
    "\n",
    "    # --- apple  ---\n",
    "    if \"apple\" in name and \"produce\" in dept:\n",
    "        return \"apple\"  \n",
    "    \n",
    "    # --- WATER  ---\n",
    "    if \"water\" in name and \"beverages\" in dept:\n",
    "        return \"water\"        \n",
    "\n",
    "    # --- YOGURT ---\n",
    "    if \"yogurt\" in name or \"yoghurt\" in name:\n",
    "        return \"yogurt\"\n",
    "\n",
    "    # --- CHEESE ---\n",
    "    if \"cheese\" in name or \"mozzarella\" in name or \"cheddar\" in name:\n",
    "        return \"cheese\"\n",
    "\n",
    "    # --- EGGS ---\n",
    "    if \"egg\" in name and \"eggplant\" not in name:\n",
    "        return \"eggs\"\n",
    "\n",
    "    # --- JUICE / SMOOTHIES ---\n",
    "    if (\n",
    "        \"juice\" in name or\n",
    "        \"smoothie\" in name or\n",
    "        \"lemonade\" in name or\n",
    "        \"cold pressed\" in name or\n",
    "        (\"beverage\" in name and (\"juice\" in name or \"pressed\" in name))\n",
    "    ):\n",
    "        return \"juice\"\n",
    "\n",
    "    # --- CEREAL / OATMEAL / GRANOLA BARS ---\n",
    "    if \"cereal\" in name or \"oatmeal\" in name or \"granola\" in name:\n",
    "        return \"cereal / oatmeal\"\n",
    "\n",
    "    # If none of the above matched, keep the cleaned name or tag as 'other'\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac86d26-4e84-4b43-bc65-5aed2525e511",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_products[\"product_group_norm\"] = frequent_products.apply(\n",
    "    normalize_product_group,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11abbce9-977e-4205-bc8f-580757e95184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sort and display the top n based on total orders\n",
    "frequent_products_df = frequent_products[[\"product_id\", \"product_name\", \"product_name_clean\", \"department\", \n",
    "                                          \"product_group_norm\", \"total_orders\",\"total_reorders\"]].sort_values('total_orders', ascending=False)\n",
    "\n",
    "frequent_products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd6589-0a7c-40e2-88d5-dc071d332a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_products.to_csv(\"frequent_products_with_department_v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737f431-4537-4c4a-9fb7-d1857f3a74bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cleaned product name\n",
    "grouped_products = (\n",
    "    frequent_products_df\n",
    "        .groupby([\"product_group_norm\", \"department\"], as_index=False)\n",
    "        .agg(\n",
    "            total_orders_sum=(\"total_orders\", \"sum\"),\n",
    "            total_reorders_sum=(\"total_reorders\", \"sum\")\n",
    "        )\n",
    ")\n",
    "\n",
    "# Recalculate reorder rate\n",
    "grouped_products[\"reorder_rate\"] = (\n",
    "    grouped_products[\"total_reorders_sum\"] /\n",
    "    grouped_products[\"total_orders_sum\"]\n",
    ")\n",
    "\n",
    "# sort by reorder_rate or total_orders\n",
    "grouped_products = grouped_products.sort_values(\n",
    "    \"total_orders_sum\", ascending=False\n",
    ")\n",
    "\n",
    "grouped_products.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73cfb8-b4ba-4462-845e-47d868e5393e",
   "metadata": {},
   "source": [
    "#### 2.1. Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718964e3-69dd-4c1c-8b04-04e03d724c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 100 products by reorder rate\n",
    "top100_by_reorder = (\n",
    "    grouped_products\n",
    "    .sort_values('reorder_rate', ascending=False)\n",
    "    .head(100)\n",
    ")\n",
    "\n",
    "# Use reorder_rate as weight\n",
    "freq_reorder = dict(\n",
    "    zip(top100_by_reorder['product_group_norm'],\n",
    "        top100_by_reorder['reorder_rate'])\n",
    ")\n",
    "\n",
    "# Create word cloud\n",
    "wc_reorder = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white'\n",
    ").generate_from_frequencies(freq_reorder)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wc_reorder, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Top 100 Products by Reorder Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec70f60-3916-4c25-ab19-cca096655e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_by_reorder.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00cb174-b263-4c3a-9201-6d5b9ba16c2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Top 100 grouped products\n",
    "top100_by_orders = (\n",
    "    grouped_products\n",
    "        .sort_values('total_orders_sum', ascending=False)\n",
    "        .head(100)\n",
    ")\n",
    "\n",
    "# Keep only the highest-order version of each normalized group\n",
    "top100_unique = (\n",
    "    top100_by_orders\n",
    "        .sort_values('total_orders_sum', ascending=False)\n",
    "        .drop_duplicates(subset='product_group_norm', keep='first')\n",
    ")\n",
    "\n",
    "# Build the frequency dictionary for WordCloud\n",
    "freq_orders = dict(\n",
    "    zip(top100_unique['product_group_norm'],\n",
    "        top100_unique['total_orders_sum'])\n",
    ")\n",
    "\n",
    "# Create WordCloud\n",
    "wc_orders = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white'\n",
    ").generate_from_frequencies(freq_orders)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wc_orders, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Top Product Groups by Total Orders')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c7d30-bad5-47a8-b251-76203f9f1cc5",
   "metadata": {},
   "source": [
    "## 3. Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d222e6-5006-434c-bc8c-6c16a628e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 500,000 orders for basket analysis\n",
    "sample_order_ids = orders_all['order_id'].drop_duplicates().sample(n=500000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8567ed53-22ff-4eb4-bbd9-b0f3356588f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = orders_all[orders_all['order_id'].isin(sample_order_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb88ea-5997-46a6-95a6-24c7fe5ef46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Top N Products (by frequency)\n",
    "top_products = sample_df['product_id'].value_counts().head(20000).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b946200c-c583-4299-bc18-72c71470a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Sampled Orders to Only Keep Top N Products\n",
    "filtered_df = sample_df[sample_df['product_id'].isin(top_products)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2a88bc-617d-4476-8eaa-dfb1d3403062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6bd09-8b22-47a8-a2b0-3eceea5aa23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Stats for the filtered dataset\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Number of orders per user\n",
    "orders_per_user_s1 = filtered_df.groupby('user_id')['order_id'].nunique()\n",
    "\n",
    "# Number of unique products per user\n",
    "products_per_user_s1 = filtered_df.groupby('user_id')['product_id'].nunique()\n",
    "\n",
    "# Number of products per order (basket size)\n",
    "products_per_order_s1 = filtered_df.groupby('order_id')['product_id'].nunique()\n",
    "\n",
    "# Reorder rate\n",
    "reorder_rate_s1 = filtered_df['reordered'].mean()\n",
    "\n",
    "# Days Since Prior Order\n",
    "days_since_prior_order_s1 = filtered_df.groupby('order_id')['days_since_prior_order'].mean()\n",
    "\n",
    "# Summarize\n",
    "stats_summary_s1 = pd.DataFrame({\n",
    "    'avg_orders_per_user': [orders_per_user_s1.mean()],\n",
    "    'avg_products_per_user': [products_per_user_s1.mean()],\n",
    "    'avg_products_per_order': [products_per_order_s1.mean()],\n",
    "    'reorder_rate': [reorder_rate_s1],\n",
    "    'total_orders': [filtered_df['order_id'].nunique()],\n",
    "    'min_orders_per_user': [orders_per_user_s1.min()],\n",
    "    'max_orders_per_user': [orders_per_user_s1.max()],\n",
    "    'total_products': [filtered_df['product_id'].nunique()],\n",
    "    'total_users': [filtered_df['user_id'].nunique()]\n",
    "})\n",
    "\n",
    "print(\"Summary Statistics for Filtered Dataset :\")\n",
    "display(stats_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b262325-f64c-4200-959c-e6d551b27411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Boxplots for the Filtered dataset\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Set a minimalist style without grid lines\n",
    "sns.set(style=\"white\", palette=\"tab10\", font_scale=1.1)\n",
    "\n",
    "# Custom color palette\n",
    "custom_colors = sns.color_palette(\"tab10\", 4)\n",
    "\n",
    "# Create figure with 4 subplots\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 6), dpi=120)\n",
    "\n",
    "# Add title\n",
    "fig.suptitle('Distribution of Key Customer and Order Metrics (Sampled Dataset)', fontsize=18, weight='bold')\n",
    "\n",
    "# Boxplot 1: Orders per User\n",
    "sns.boxplot(y=orders_per_user_s1, ax=axes[0], color=custom_colors[0], width=0.5, showfliers=True)\n",
    "axes[0].set_title('Orders per User', fontsize=14)\n",
    "axes[0].set_ylabel('Number of Orders', fontsize=20, labelpad=10)\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].tick_params(axis='y', labelsize=18)\n",
    "axes[0].spines[['top', 'right']].set_visible(False)\n",
    "axes[0].grid(False)\n",
    "\n",
    "# Boxplot 2: Unique Products per User\n",
    "sns.boxplot(y=products_per_user_s1, ax=axes[1], color=custom_colors[1], width=0.5, showfliers=True)\n",
    "axes[1].set_title('Unique Products per User', fontsize=14)\n",
    "axes[1].set_ylabel('Number of Unique Products', fontsize=20, labelpad=10)\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].tick_params(axis='y', labelsize=18)\n",
    "axes[1].spines[['top', 'right']].set_visible(False)\n",
    "axes[1].grid(False)\n",
    "\n",
    "# Boxplot 3: Products per Order (Basket Size)\n",
    "sns.boxplot(y=products_per_order_s1, ax=axes[2], color=custom_colors[2], width=0.5, showfliers=True)\n",
    "axes[2].set_title('Products per Order (Basket Size)', fontsize=14)\n",
    "axes[2].set_ylabel('Number of Products', fontsize=20, labelpad=10)\n",
    "axes[2].set_xlabel('')\n",
    "axes[2].tick_params(axis='y', labelsize=18)\n",
    "axes[2].spines[['top', 'right']].set_visible(False)\n",
    "axes[2].grid(False)\n",
    "\n",
    "# Boxplot 4: Days Since Prior Order\n",
    "sns.boxplot(y=days_since_prior_order_s1, ax=axes[3], color=custom_colors[3], width=0.5, showfliers=True)\n",
    "axes[3].set_title('Days Since Prior Order', fontsize=14)\n",
    "axes[3].set_ylabel('Number of Days', fontsize=20, labelpad=10)\n",
    "axes[3].set_xlabel('')\n",
    "axes[3].tick_params(axis='y', labelsize=18)\n",
    "axes[3].spines[['top', 'right']].set_visible(False)\n",
    "axes[3].grid(False)\n",
    "\n",
    "# Layout adjustments\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e8939d-b9c3-4944-b6e3-aa7902e669ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Stats for the sampled dataset\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Create a Dictionary of Datasets\n",
    "datasets_s1 = {\n",
    "    'Orders per User': orders_per_user_s1,\n",
    "    'Products per User': products_per_user_s1, \n",
    "    'Products per Order': products_per_order_s1, \n",
    "    'Days since Prior order': days_since_prior_order_s1\n",
    "}    \n",
    "    \n",
    "# Generate describe() for each and store results\n",
    "summary_dict_s1 = {}\n",
    "for name, df in datasets_s1.items():\n",
    "    desc = df.describe()\n",
    "    summary_dict_s1[name] = desc\n",
    "\n",
    "# Concatenate along columns with hierarchical index\n",
    "summary_df_s1 = pd.concat(summary_dict_s1, axis=1)\n",
    "formatted_summary_s1 = summary_df_s1.applymap(lambda x: f\"{x:,.2f}\")\n",
    "\n",
    "# View metrics as rows and datasets as columns\n",
    "formatted_summary_s1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adabe9f0-3e55-40cd-8ff3-76f82804f1ec",
   "metadata": {},
   "source": [
    "## 4. Customer Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342cfbb-d730-4a80-a987-8ac515ca14fa",
   "metadata": {},
   "source": [
    "### 4.1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778a80f-3d31-43b1-92b9-367f5437dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate Reorder Rate per User and Department\n",
    "user_dept_reorder_rate = (\n",
    "    filtered_df.groupby(['user_id', 'department_id'])['reordered']\n",
    "    .mean()\n",
    "    .reset_index(name='reorder_rate')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff387677-abf7-494b-99ae-7a2079943f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pivot to get User-Department Matrix (Features for Clustering)\n",
    "user_dept_matrix = user_dept_reorder_rate.pivot(\n",
    "    index='user_id',\n",
    "    columns='department_id',\n",
    "    values='reorder_rate'\n",
    ").fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c78777-ce57-47cf-a5e1-c64a1d06c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Standardize the data\n",
    "scaler = StandardScaler()\n",
    "user_scaled = scaler.fit_transform(user_dept_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff36aa-2df5-45b6-bec6-a60af53f201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. PCA dimensionality reduction (2D for clustering visualization + elbow)\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(user_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a40572-e248-4e4f-89a3-b8b04481aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Determine number of clusters using the Elbow Method\n",
    "kmeans_kwargs = {\n",
    "    \"init\": \"random\",\n",
    "    \"n_init\": 8,\n",
    "    \"max_iter\": 300,\n",
    "    \"random_state\": 28,\n",
    "}\n",
    "\n",
    "sse = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(principal_components)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(1, 11), sse)\n",
    "plt.xticks(range(1, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec81eb-dbcb-47eb-a515-c5fce39192ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Perform K-Means Clustering with chosen K\n",
    "K = 5 ## Based on the Elbow method above\n",
    "kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
    "user_dept_matrix['cluster'] = kmeans.fit_predict(user_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3dca63-763f-40a0-993d-606f58aaa763",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dept_matrix.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be9721-886d-48bb-ae74-40d28cd0798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Analyze Cluster Characteristics\n",
    "user_dept_matrix.columns = user_dept_matrix.columns.astype(str)\n",
    "department_columns_str = user_dept_matrix.columns.drop('cluster').tolist()\n",
    "\n",
    "agg_dict = {col: (col, 'mean') for col in department_columns_str}\n",
    "agg_dict['total_users'] = ('cluster', 'count')\n",
    "\n",
    "cluster_summary = user_dept_matrix.groupby('cluster').agg(**agg_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e6963-ace1-431d-9928-e4b7e8d7fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Prepare summary for visualization\n",
    "display_summary = cluster_summary.copy()\n",
    "rate_cols = [c for c in display_summary.columns if c != 'total_users']\n",
    "display_summary[rate_cols] = display_summary[rate_cols] * 100\n",
    "\n",
    "display_summary = display_summary.reset_index().rename(columns={'cluster': 'Segment ID'})\n",
    "display_summary_formatted = display_summary.set_index('Segment ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864bf35c-8115-4144-825f-37eebe40e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'departments' in globals():\n",
    "    dept_map = departments.set_index('department_id')['department'].to_dict()\n",
    "    new_cols = {}\n",
    "    for original, col in zip(department_columns_str, department_columns_str):\n",
    "        dept_name = dept_map.get(int(original), f\"ID_{original}\")\n",
    "        new_cols[original] = f\"mean_reorder_rate_{dept_name}\"\n",
    "    display_summary_formatted = display_summary_formatted.rename(columns=new_cols, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f1580-9747-415b-95e5-1100e4f1b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3438efa-546f-4fbf-97fb-de6325f3a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. PCA Visualization\n",
    "pca_df = pd.DataFrame(principal_components, columns=['PC1', 'PC2'])\n",
    "pca_df['Cluster'] = user_dept_matrix['cluster'].values\n",
    "\n",
    "centroids_scaled = kmeans.cluster_centers_\n",
    "centroids_pca = pca.transform(centroids_scaled)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "sns.scatterplot(\n",
    "    x='PC1', y='PC2', hue='Cluster', data=pca_df,\n",
    "    palette=sns.color_palette(\"tab10\", n_colors=K),\n",
    "    s=40, alpha=0.4\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    centroids_pca[:, 0], centroids_pca[:, 1],\n",
    "    marker='D', s=350, color='gold',\n",
    "    edgecolors='black', linewidth=2.0\n",
    ")\n",
    "\n",
    "for i, center in enumerate(centroids_pca):\n",
    "    plt.text(center[0], center[1], f'C{i}',\n",
    "             fontsize=14, weight='heavy',\n",
    "             ha='center', va='center',\n",
    "             bbox=dict(facecolor='white', alpha=0.8, edgecolor='black')\n",
    ")\n",
    "\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d7b5f-b1ff-4040-931d-ac866f5b632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Compute centroid-to-centroid distances\n",
    "dist_matrix = euclidean_distances(centroids)\n",
    "\n",
    "# Make it a readable DataFrame\n",
    "dist_df = pd.DataFrame(dist_matrix, \n",
    "                       columns=[f\"C{c}\" for c in range(K)], \n",
    "                       index=[f\"C{c}\" for c in range(K)])\n",
    "\n",
    "dist_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a86f5-bae0-478e-84c1-e919b73ca4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(dist_df, annot=True, cmap=\"Blues\", fmt=\".2f\")\n",
    "plt.title(\"Centroid Distance Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7702e681-10a5-46c6-bfed-1017c9fdb5b8",
   "metadata": {},
   "source": [
    "### 4.2. Merge prod Norm with Filtered DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd7c27d-6040-4b9e-ac6d-f4eecd9c3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns wanted to bring into filtered_df\n",
    "freq_cols = frequent_products_df[[\n",
    "    \"product_id\",\n",
    "    \"product_name_clean\",\n",
    "    \"product_group_norm\"\n",
    "]]\n",
    "\n",
    "# Merge filtered_df with the cleaned product info\n",
    "filtered_df = filtered_df.merge(\n",
    "    freq_cols,\n",
    "    on=\"product_id\",\n",
    "    how=\"left\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e0f63a-1d54-49e4-9783-3cf4963035a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"product_group_norm\"] = filtered_df[\"product_group_norm\"].fillna(\"other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878cf3e-761d-4961-a056-4be4467ffb6a",
   "metadata": {},
   "source": [
    "### 4.3. Efficient Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed0c77-38fa-459b-bf07-61d104075171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add info\n",
    "filtered_df = filtered_df.merge(\n",
    "    user_dept_matrix['cluster'],\n",
    "    left_on='user_id',\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de069a-a747-459b-92bd-1d1c507b8ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by Segment 4\n",
    "segment_4_df = filtered_df[filtered_df['cluster'] == 3].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc3fbb-7e44-4200-9b22-1a038e38b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of transactions\n",
    "transactions = (segment_4_df.groupby('order_id')['product_id']\n",
    "                .apply(list)\n",
    "                .tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38daeeb5-4a0e-4e5a-bc86-7249aef03e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "start_time = time.time()\n",
    "print(\"Apriori started...\")\n",
    "print(f\"Start time: {time.ctime(start_time)}\")\n",
    "\n",
    "# Run Apriori\n",
    "itemsets, rules = apriori(transactions, min_support=0.005, min_confidence=0.3)\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    " \n",
    "# Calculate duration\n",
    "duration = end_time - start_time\n",
    "\n",
    "# Log results\n",
    "print(\"Apriori completed.\")\n",
    "print(f\"End time:   {time.ctime(end_time)}\")\n",
    "print(f\"Duration:   {duration:.2f} seconds ({duration/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355a0064-02c3-4f8b-b2de-cfa1950f2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets:\", len(itemsets))\n",
    "print(\"Rules generated:\", len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853ceae8-14c5-4ae1-930e-95d01526ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemsets is a dictionary\n",
    "for length in sorted(itemsets):\n",
    "    print(f\"{length}-itemsets: {len(itemsets[length])}\")\n",
    "\n",
    "# Total number of frequent itemsets\n",
    "total_itemsets = sum(len(v) for v in itemsets.values())\n",
    "print(f\"\\nTotal frequent itemsets: {total_itemsets}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cbc73c-e541-49e2-80f7-6dd07333514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter itemsets of size 2 or more\n",
    "records = []\n",
    "for k, v in itemsets.items():\n",
    "    if k >= 2:\n",
    "        for product_ids, support in v.items():\n",
    "            records.append({'itemset': product_ids, 'support': support, 'length': k})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_frequent = pd.DataFrame(records)\n",
    "\n",
    "# Function to map product_ids to names\n",
    "def map_names(product_ids, product_map):\n",
    "    return [product_map.get(pid, f\"Unknown({pid})\") for pid in product_ids]\n",
    "\n",
    "# Create product_id to name mapping\n",
    "product_map = dict(zip(products['product_id'], products['product_name']))\n",
    "\n",
    "# Add readable product names column\n",
    "df_frequent['product_names'] = df_frequent['itemset'].apply(lambda x: map_names(x, product_map))\n",
    "\n",
    "print(df_frequent[['product_names', 'support', 'length']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7adf6-518b-458e-ba39-85a4585b047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_map = products.set_index(\"product_id\")[\"product_name\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce222f1c-e69d-4511-a5bf-32f2d8417eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules\n",
    "rules_records = []\n",
    "\n",
    "for rule in rules:\n",
    "    rules_records.append({\n",
    "        \"antecedent_ids\": list(rule.lhs),\n",
    "        \"consequent_ids\": list(rule.rhs),\n",
    "        \"support\": rule.support,\n",
    "        \"confidence\": rule.confidence,\n",
    "        \"lift\": rule.lift\n",
    "    })\n",
    "\n",
    "rules_products_df = pd.DataFrame(rules_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e05f2e-a7da-450e-b317-9939e4623a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules\n",
    "rules_products_df[\"antecedent_names\"] = rules_products_df[\"antecedent_ids\"].apply(\n",
    "    lambda ids: [product_map[i] for i in ids]\n",
    ")\n",
    "\n",
    "rules_products_df[\"consequent_names\"] = rules_products_df[\"consequent_ids\"].apply(\n",
    "    lambda ids: [product_map[i] for i in ids]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152320bc-bcb1-4f4f-96b7-deaac28f6672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules\n",
    "rules_products_df = rules_products_df[\n",
    "    [\n",
    "        \"antecedent_ids\", \"antecedent_names\",\n",
    "        \"consequent_ids\", \"consequent_names\",\n",
    "        \"support\", \"confidence\", \"lift\"\n",
    "    ]\n",
    "].sort_values(\"confidence\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3b93a-1881-4079-8dd7-b7b72aaa52a8",
   "metadata": {},
   "source": [
    "## 5. Association analysis - Aisle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce5cb76-2c31-43e6-a485-8b7d8a7f6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of transactions\n",
    "transactions_aisle = (filtered_df.groupby('order_id')['aisle_id']\n",
    "                .apply(list)\n",
    "                .tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc219fa-a40f-47b3-9f96-c06e5bc22220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "start_time = time.time()\n",
    "print(\"Apriori started...\")\n",
    "print(f\"Start time: {time.ctime(start_time)}\")\n",
    "\n",
    "# Run Apriori\n",
    "itemsets, rules = apriori(transactions_aisle, min_support=0.010, min_confidence=0.3)\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    " \n",
    "# Calculate duration\n",
    "duration = end_time - start_time\n",
    "\n",
    "# Log results\n",
    "print(\"Apriori completed.\")\n",
    "print(f\"End time:   {time.ctime(end_time)}\")\n",
    "print(f\"Duration:   {duration:.2f} seconds ({duration/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76ca53-ffa4-4b02-89ab-eac5546d28a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only frequent itemsets of size 2 or more\n",
    "\n",
    "min_size = 2 \n",
    "records = []\n",
    "total_transactions = len(transactions_aisle)  # needed for support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f441c4-b78f-4634-a23d-b7eee910ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each size group\n",
    "for k, item_dict in itemsets.items():\n",
    "    if k < min_size:\n",
    "        continue\n",
    "    for items, count in item_dict.items():\n",
    "        records.append({\n",
    "            \"itemset_ids\": list(items),\n",
    "            \"support_count\": count,\n",
    "            \"support\": count / total_transactions\n",
    "        })\n",
    "\n",
    "itemsets_df = pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dae1ea7-28c1-42a6-b168-3dc1c7bd4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IDs to names\n",
    "aisle_map = aisles.set_index(\"aisle_id\")[\"aisle\"].to_dict()\n",
    "\n",
    "itemsets_df[\"itemset_names\"] = itemsets_df[\"itemset_ids\"].apply(\n",
    "    lambda ids: [aisle_map[i] for i in ids]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bfa32b-c6f0-44f9-b060-39221e6dae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsets_df['Len'] = itemsets_df['itemset_names'].apply(\n",
    "    lambda x: len(ast.literal_eval(x)) if isinstance(x, str) else len(x)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194963f9-d3c5-4158-9ef4-3b3d57072011",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6e070-300a-4d5f-a1ae-ac93f6025cb3",
   "metadata": {},
   "source": [
    "### 5.1. Aisle Network Graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892fc53-24c5-470f-88e3-64e04f307183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Take top_n itemsets and keep only pairs\n",
    "top_n = 20\n",
    "df = (\n",
    "    itemsets_df\n",
    "    .sort_values(\"support\", ascending=False)\n",
    "    .head(top_n)\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# Ensure Len exists and/or filter to length 2\n",
    "df['Len'] = df['itemset_names'].apply(\n",
    "    lambda x: len(ast.literal_eval(x)) if isinstance(x, str) else len(x)\n",
    ")\n",
    "df_pairs = df[df['Len'] == 2].copy()\n",
    "\n",
    "# 2. Build node supports and edges from *pairs only*\n",
    "edge_weights = {}\n",
    "node_supports = {}\n",
    "\n",
    "for _, row in df_pairs.iterrows():\n",
    "    items = row['itemset_names']\n",
    "    if isinstance(items, str):\n",
    "        items = ast.literal_eval(items)\n",
    "    a, b = sorted([str(i).strip() for i in items])\n",
    "    supp = row['support']\n",
    "\n",
    "    # edge weight = EXACT support from the table row\n",
    "    edge_weights[(a, b)] = supp\n",
    "\n",
    "    # node support = sum of supports of its pair itemsets\n",
    "    for item in (a, b):\n",
    "        node_supports[item] = node_supports.get(item, 0) + supp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96797985-0a0e-41ae-b97d-c1d17f8d2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build graph\n",
    "G = nx.Graph()\n",
    "for item, s in node_supports.items():\n",
    "    G.add_node(item, support=s)\n",
    "\n",
    "for (a, b), w in edge_weights.items():\n",
    "    G.add_edge(a, b, weight=w)\n",
    "\n",
    "# 4. Dynamic node sizes based on support\n",
    "node_sizes = [2300 + node_supports[n] * 7000 for n in G.nodes()]\n",
    "\n",
    "edge_labels = {(u, v): f\"{G[u][v]['weight']*100:.2f}%\" for u, v in G.edges()}\n",
    "\n",
    "edge_widths = [G[u][v]['weight'] * 20 for u, v in G.edges()] \n",
    "\n",
    "wrapped_labels = {\n",
    "    n: \"\\n\".join(textwrap.wrap(n, width=10))  # adjust width as needed\n",
    "    for n in G.nodes()\n",
    "}\n",
    "\n",
    "# 5. Layout + plot\n",
    "fig = plt.figure(figsize=(12, 10), facecolor=\"white\")  # white figure background\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor(\"white\")                            # white axes background\n",
    "\n",
    "pos = nx.spring_layout(G, k=10, scale=10, iterations=300, seed=42)\n",
    "\n",
    "# nodes not transparent (alpha=1)\n",
    "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=\"#105054\", alpha=1.0)\n",
    "nx.draw_networkx_edges(G, pos, width=edge_widths, edge_color=\"gray\", alpha=0.6)\n",
    "\n",
    "# use wrapped labels\n",
    "nx.draw_networkx_labels(G, pos, labels=wrapped_labels, font_size=14, font_color=\"white\", font_weight=\"bold\")\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=12, label_pos=0.5)\n",
    "\n",
    "# no title\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796be21-7020-4669-82e0-6e3808dcf6f9",
   "metadata": {},
   "source": [
    "### 5.2. Aisle Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13cb20-eb33-4fc4-b5de-8fb72e025c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ID to name dictionary\n",
    "aisle_map = aisles.set_index(\"aisle_id\")[\"aisle\"].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752a74c-5c94-4905-b310-287782a8d607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules\n",
    "rules_records = []\n",
    "\n",
    "for rule in rules:\n",
    "    rules_records.append({\n",
    "        \"antecedent_ids\": list(rule.lhs),\n",
    "        \"consequent_ids\": list(rule.rhs),\n",
    "        \"support\": rule.support,\n",
    "        \"confidence\": rule.confidence,\n",
    "        \"lift\": rule.lift\n",
    "    })\n",
    "\n",
    "rules_df = pd.DataFrame(rules_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d1f9d4-5a02-4528-bd0c-6245c688c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IDs to aisle names using the aisle_map\n",
    "rules_df[\"antecedent_names\"] = rules_df[\"antecedent_ids\"].apply(\n",
    "    lambda ids: [aisle_map[i] for i in ids]\n",
    ")\n",
    "\n",
    "rules_df[\"consequent_names\"] = rules_df[\"consequent_ids\"].apply(\n",
    "    lambda ids: [aisle_map[i] for i in ids]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f8b6f-1a31-419b-bb8b-e93d5558f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules\n",
    "rules_df = rules_df[\n",
    "    [\n",
    "        \"antecedent_ids\", \"antecedent_names\",\n",
    "        \"consequent_ids\", \"consequent_names\",\n",
    "        \"support\", \"confidence\", \"lift\"\n",
    "    ]\n",
    "].sort_values(\"confidence\", ascending=False)\n",
    "\n",
    "rules_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5bac4-b5f1-4e8d-a6d7-4540a640a91f",
   "metadata": {},
   "source": [
    "## 6. Association Analysis Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4ca982-01d1-4e64-a15e-44d1725ce3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of transactions\n",
    "transactions_dept = (filtered_df.groupby('order_id')['department_id']\n",
    "                .apply(list)\n",
    "                .tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb5a95-9b3a-47b6-b7fa-b2a6ed0c9808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "start_time = time.time()\n",
    "print(\"Apriori started...\")\n",
    "print(f\"Start time: {time.ctime(start_time)}\")\n",
    "\n",
    "# Run Apriori\n",
    "itemsets, rules = apriori(transactions_dept, min_support=0.010, min_confidence=0.3)\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    " \n",
    "# Calculate duration\n",
    "duration = end_time - start_time\n",
    "\n",
    "# Log results\n",
    "print(\"Apriori completed.\")\n",
    "print(f\"End time:   {time.ctime(end_time)}\")\n",
    "print(f\"Duration:   {duration:.2f} seconds ({duration/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa0caf1-0fa3-452f-a265-c319e4f36bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only frequent itemsets of size 2 or more\n",
    "\n",
    "min_size = 2 \n",
    "records = []\n",
    "total_transactions = len(transactions_dept)  # needed for support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e9dd7-b5ca-4825-a051-5bf3d0d71878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each size group\n",
    "for k, item_dict in itemsets.items():\n",
    "    if k < min_size:\n",
    "        continue\n",
    "    for items, count in item_dict.items():\n",
    "        records.append({\n",
    "            \"itemset_ids\": list(items),\n",
    "            \"support_count\": count,\n",
    "            \"support\": count / total_transactions\n",
    "        })\n",
    "\n",
    "dept_itemsets_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a18189-6f41-4c50-b90c-fa718fe880ed",
   "metadata": {},
   "source": [
    "### 6.1. Rules Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa08aeb2-2a18-4196-b7bd-f079b0a76eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ID to name dictionary\n",
    "dept_map = departments.set_index(\"department_id\")[\"department\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a806e1b-2cb4-4c13-a9a7-8edcae5c1086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules\n",
    "rules_records = []\n",
    "\n",
    "for rule in rules:\n",
    "    rules_records.append({\n",
    "        \"antecedent_ids\": list(rule.lhs),\n",
    "        \"consequent_ids\": list(rule.rhs),\n",
    "        \"support\": rule.support,\n",
    "        \"confidence\": rule.confidence,\n",
    "        \"lift\": rule.lift\n",
    "    })\n",
    "\n",
    "rules_df = pd.DataFrame(rules_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2408c3-e6ea-417f-9ce9-10a43065372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IDs to aisle names using your aisle_map\n",
    "rules_df[\"antecedent_names\"] = rules_df[\"antecedent_ids\"].apply(\n",
    "    lambda ids: [dept_map[i] for i in ids]\n",
    ")\n",
    "\n",
    "rules_df[\"consequent_names\"] = rules_df[\"consequent_ids\"].apply(\n",
    "    lambda ids: [dept_map[i] for i in ids]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8173e8f1-40f9-4cd7-ad90-352d4339bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules\n",
    "rules_df = rules_df[\n",
    "    [\n",
    "        \"antecedent_ids\", \"antecedent_names\",\n",
    "        \"consequent_ids\", \"consequent_names\",\n",
    "        \"support\", \"confidence\", \"lift\"\n",
    "    ]\n",
    "].sort_values(\"confidence\", ascending=False)\n",
    "\n",
    "rules_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db01fa94-2bcd-4ce8-9972-4a0eed49b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ea0ba-cb27-49c3-94f7-7abd27495484",
   "metadata": {},
   "source": [
    "### 6.2. Department Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e82692d-2e1e-41c2-990d-b1a6acaa5454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID to department name map\n",
    "dept_map = departments.set_index(\"department_id\")[\"department\"].to_dict()\n",
    "\n",
    "# Add the missing column\n",
    "dept_itemsets_df[\"itemset_names\"] = dept_itemsets_df[\"itemset_ids\"].apply(\n",
    "    lambda ids: [dept_map[i] for i in ids]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79e7d4-6724-40b2-91f7-9a5bc28e2f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_itemsets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ba5cd-f4ad-4470-9107-0115774bea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Department network graph\n",
    "\n",
    "df_dept = dept_itemsets_df.copy()\n",
    "\n",
    "# Keep only top_n itemsets\n",
    "top_n = 20\n",
    "df_dept = df_dept.sort_values(\"support\", ascending=False).head(top_n)\n",
    "\n",
    "# Keep only itemsets of length 2\n",
    "df_dept[\"Len\"] = df_dept[\"itemset_names\"].apply(lambda x: len(x))\n",
    "df_pairs = df_dept[df_dept[\"Len\"] == 2].copy()\n",
    "\n",
    "# Build edges + node supports\n",
    "edge_weights = {}\n",
    "node_supports = {}\n",
    "\n",
    "for _, row in df_pairs.iterrows():\n",
    "    items = row[\"itemset_names\"]\n",
    "    \n",
    "    a, b = sorted(items)\n",
    "    supp = row[\"support\"]\n",
    "\n",
    "    # edge weight\n",
    "    edge_weights[(a, b)] = supp\n",
    "\n",
    "    # node support\n",
    "    for item in (a, b):\n",
    "        node_supports[item] = node_supports.get(item, 0) + supp\n",
    "\n",
    "# Build graph\n",
    "G = nx.Graph()\n",
    "\n",
    "for item, s in node_supports.items():\n",
    "    G.add_node(item, support=s)\n",
    "\n",
    "for (a, b), w in edge_weights.items():\n",
    "    G.add_edge(a, b, weight=w)\n",
    "\n",
    "# Node sizes + edge widths\n",
    "node_sizes = [2000 + node_supports[n] * 3000 for n in G.nodes()]\n",
    "edge_widths = [G[u][v][\"weight\"] * 20 for u, v in G.edges()]\n",
    "\n",
    "# Edge labels\n",
    "edge_labels = {(u, v): f\"{G[u][v]['weight']*100:.2f}%\"\n",
    "               for u, v in G.edges()}\n",
    "\n",
    "# Wrap labels\n",
    "wrapped_labels = {\n",
    "    n: \"\\n\".join(textwrap.wrap(n, width=12))\n",
    "    for n in G.nodes()\n",
    "}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10), facecolor=\"white\")\n",
    "\n",
    "pos = nx.spring_layout(G, k=10, scale=10, iterations=300, seed=42)\n",
    "\n",
    "# move \"dairy eggs\" a bit\n",
    "if \"dairy eggs\" in pos:\n",
    "    x, y = pos[\"dairy eggs\"]\n",
    "    pos[\"dairy eggs\"] = (x - 0.2, y + 0.6) \n",
    "\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos,\n",
    "                       node_size=node_sizes,\n",
    "                       node_color=\"#002D72\",\n",
    "                       alpha=1.0)\n",
    "\n",
    "nx.draw_networkx_edges(G, pos,\n",
    "                       width=edge_widths,\n",
    "                       edge_color=\"gray\",\n",
    "                       alpha=0.6)\n",
    "\n",
    "nx.draw_networkx_labels(G, pos,\n",
    "                        labels=wrapped_labels,\n",
    "                        font_size=14,\n",
    "                        font_color=\"white\",\n",
    "                        font_weight=\"bold\")\n",
    "\n",
    "nx.draw_networkx_edge_labels(G, pos,\n",
    "                             edge_labels=edge_labels,\n",
    "                             font_size=12)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d327f-8e20-482c-92b4-d7c81384eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef21bd6-412d-428f-a348-c03bae488f1f",
   "metadata": {},
   "source": [
    "## 7. Sequential pattern mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0df90-60dd-4af4-8bf4-c2791cd40488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment 4 \n",
    "segment_4_seq_df = segment_4_df[['user_id', 'order_id', 'aisle_id']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac663956-bf19-4d0c-92d2-f84e34eba74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment 4\n",
    "segment_4_seq_df = segment_4_seq_df.sort_values(['user_id', 'order_id'])\n",
    "\n",
    "segment_4_seq_df['rank'] = (\n",
    "    segment_4_seq_df.groupby('user_id')['order_id']\n",
    "    .rank(method='dense')\n",
    "    .astype(int)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451302c9-2386-4ef2-ad39-66a977aa83ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Order-level: for each (user, rank) get list of aisles in that order\n",
    "order_level = (\n",
    "    segment_4_seq_df\n",
    "    .groupby(['user_id', 'rank'])['aisle_id']\n",
    "    .apply(list)\n",
    ")\n",
    "\n",
    "# 2) User-level sequences: each user is a list of orders (each order is a list of aisles)\n",
    "user_sequences = (\n",
    "    order_level\n",
    "    .groupby(level=0)   # group by user_id (index level 0)\n",
    "    .apply(list)\n",
    "    .tolist()           # final list of sequences for PrefixSpan\n",
    ")\n",
    "\n",
    "print(f\"Number of users (sequences): {len(user_sequences)}\")\n",
    "print(\"Example sequence for one user:\\n\", user_sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1731c93-0722-4083-a1a2-99881134f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences\n",
    "user_sequences = [\n",
    "    [tuple(order) for order in sequence]\n",
    "    for sequence in user_sequences\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1892c837-a7a3-42bd-85ae-3168e9175bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each order's aisle list into a tuple (hashable)\n",
    "user_sequences = [\n",
    "    [tuple(order) for order in user_orders]\n",
    "    for user_orders in user_sequences\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02db0ac-2f3b-4704-ab3e-149b4edeb276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PrefixSpan\n",
    "ps = PrefixSpan(user_sequences)\n",
    "\n",
    "# Pattern length constraints\n",
    "ps.minlen = 2       # allow single-step patterns\n",
    "ps.maxlen = 4       \n",
    "\n",
    "# Support threshold tuning\n",
    "total_users = len(user_sequences)\n",
    "\n",
    "# Try 0.1% support\n",
    "minsup_fraction = 0.001\n",
    "minsup = max(2, int(total_users * minsup_fraction))\n",
    "print(\"Using minsup:\", minsup)\n",
    "\n",
    "# Mine patterns\n",
    "patterns = ps.frequent(minsup)\n",
    "\n",
    "# Sort by support\n",
    "patterns = sorted(patterns, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Show results\n",
    "for sup, seq in patterns[:15]:\n",
    "    print(f\"support={sup}/{total_users} ({sup/total_users:.3%})  sequence={seq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be5892-9018-4bef-b15a-0c38f21c9421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map aisle_id to aisle_name\n",
    "aisle_map = dict(zip(aisles['aisle_id'], aisles['aisle']))\n",
    "\n",
    "total_users = len(user_sequences)\n",
    "\n",
    "def seq_to_text(seq):\n",
    "    \"\"\"\n",
    "    seq is something like: [(84,), (83,), (120, 123)]\n",
    "    Return: 'fresh fruits  fresh vegetables  packaged vegetables + yogurt'\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    for order in seq:\n",
    "        # order is a tuple of aisle_ids\n",
    "        labels = [aisle_map.get(i, str(i)) for i in order]\n",
    "        steps.append(\" + \".join(labels))   # aisles in the same order\n",
    "    return \"  \".join(steps)               # sequence across orders\n",
    "\n",
    "rows = []\n",
    "for sup, seq in patterns[:50]:   # take top 50\n",
    "    rows.append({\n",
    "        \"support_abs\": sup,\n",
    "        \"support_pct\": sup / total_users,\n",
    "        \"length\": len(seq),\n",
    "        \"sequence_text\": seq_to_text(seq)\n",
    "    })\n",
    "\n",
    "patterns_df = pd.DataFrame(rows).sort_values(\"support_pct\", ascending=False)\n",
    "\n",
    "patterns_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a436af-1f43-430b-bfce-96dffc9134d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendation function\n",
    " \n",
    "def get_next_order_recommendations(last_purchased_aisle, patterns_df, top_n=3):\n",
    "    \"\"\"\n",
    "    Recommends aisles for the next order based on sequential patterns.\n",
    " \n",
    "    :param last_purchased_aisle: The aisle_id the user bought in their last order.\n",
    "    :param patterns_df: The DataFrame of sequential patterns (aisle_A -> aisle_B).\n",
    "    :param top_n: The number of top recommendations to return.\n",
    "    :return: A list of recommended aisles (aisle_id).\n",
    "    \"\"\"\n",
    "    # Filter for patterns where aisle_A matches the last purchased aisle\n",
    "    recommendations = patterns_df[patterns_df['aisle_A'] == last_purchased_aisle].copy()\n",
    " \n",
    "    if recommendations.empty:\n",
    "        return []\n",
    " \n",
    "    # Calculate 'Confidence' (simplified as P(B|A) = count(A->B) / count(A))\n",
    "    # This represents the probability that B will be bought next, given A was bought now.\n",
    "    count_A = patterns_df[patterns_df['aisle_A'] == last_purchased_aisle]['support_count'].sum()\n",
    "    recommendations['confidence'] = recommendations['support_count'] / count_A\n",
    " \n",
    "    # Sort by confidence and get the top N\n",
    "    recommendations = recommendations.sort_values(by='confidence', ascending=False)\n",
    " \n",
    "    return recommendations['aisle_B'].head(top_n).tolist()\n",
    "\n",
    " \n",
    "# 2. Data Preparation: Structure data into sequences of itemsets (orders)\n",
    "# Ensure data is sorted chronologically\n",
    "df_orders = segment_4_seq_df.sort_values(['user_id', 'order_id'])\n",
    " \n",
    "# Group all AISLE_IDs purchased in the same order into a list (the 'itemset')\n",
    "df_sequences = df_orders.groupby(['user_id', 'order_id'])['aisle_id'].apply(list).reset_index(name='itemset')\n",
    " \n",
    "# 3. Create transitions from Itemset N to Itemset N+1\n",
    "# The .shift(-1) operation brings the next order's itemset into the current row\n",
    "df_sequences['next_order_id'] = df_sequences.groupby('user_id')['order_id'].shift(-1)\n",
    "df_sequences['next_itemset'] = df_sequences.groupby('user_id')['itemset'].shift(-1)\n",
    " \n",
    "# Drop the last order for each user as it has no 'next_itemset'\n",
    "df_transitions = df_sequences.dropna(subset=['next_itemset']).copy()\n",
    " \n",
    "# 4. get all sequential item pairs (Aisle A in Order N -> Aisle B in Order N+1)\n",
    "# creates a row for every combination of (Aisle A in current order, Aisle B in next order)\n",
    "df_transitions_A = df_transitions.explode('itemset').rename(columns={'itemset': 'aisle_A'})\n",
    "df_transitions_final = df_transitions_A.explode('next_itemset').rename(columns={'next_itemset': 'aisle_B'})\n",
    "\n",
    "df_transitions_final = df_transitions_final[df_transitions_final['aisle_A'] != df_transitions_final['aisle_B']]\n",
    " \n",
    "# 5. Sequential Pattern Mining (Count frequency/support)\n",
    "# core of recommendation system\n",
    "sequential_patterns = df_transitions_final.groupby(['aisle_A', 'aisle_B']).size().reset_index(name='support_count')\n",
    "sequential_patterns = sequential_patterns.sort_values(by='support_count', ascending=False).reset_index(drop=True)\n",
    " \n",
    "# Recommendation example\n",
    "user_last_item = 40 \n",
    "recommendations = get_next_order_recommendations(user_last_item, sequential_patterns)\n",
    " \n",
    "print(\"\\n--- Sequential Pattern Mining Results (Top 10) ---\")\n",
    "print(sequential_patterns.head(20))\n",
    " \n",
    "print(f\"\\nIf a customer bought items from 'aisle_id' **{user_last_item}** in their last order,\")\n",
    "print(f\"the top recommendations (next aisles) for their next order are: **{recommendations}**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a279890-7af3-4150-b429-f4472d5971f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_patterns.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c57e0-f91e-417f-b598-d79821b5ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendation example\n",
    "user_last_item = 123 \n",
    "recommendations = get_next_order_recommendations(user_last_item, sequential_patterns)\n",
    " \n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" Sequential Pattern Mining Analysis (Aisle Transitions)\")\n",
    "print(\"=\"*70)\n",
    " \n",
    "# 1. Display the Core Sequential Patterns Table (Top 10)\n",
    "print(\"\\n Top 10 Sequential Aisle Patterns (Support Count)\")\n",
    "print(\"This shows the most frequent transitions from one aisle (A) to another (B) in subsequent orders.\")\n",
    " \n",
    "# Rename columns for display clarity\n",
    "display_patterns = sequential_patterns.head(5).copy()\n",
    "display_patterns.columns = ['Aisle_A (Previous Order)', 'Aisle_B (Next Order)', 'Transition Count (Support)']\n",
    " \n",
    "print(display_patterns.to_markdown(index=False))\n",
    " \n",
    "# 2. Display the Specific Recommendations and Confidence\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(f\" Next-Order Recommendations for Aisle ID: {user_last_item}\")\n",
    "print(\"-\" * 70)\n",
    " \n",
    "if recommendations:\n",
    "    # Recalculate recommendations dataframe to show confidence for display\n",
    "    rec_df = sequential_patterns[sequential_patterns['aisle_A'] == user_last_item].copy()\n",
    "    # Calculate Confidence\n",
    "    count_A = sequential_patterns[sequential_patterns['aisle_A'] == user_last_item]['support_count'].sum()\n",
    "    rec_df['confidence'] = rec_df['support_count'] / count_A\n",
    "    # Sort and format for output\n",
    "    rec_df = rec_df.sort_values(by='confidence', ascending=False)\n",
    "    rec_df = rec_df.rename(columns={'aisle_A': 'Current Aisle (A)', 'aisle_B': 'Recommended Next Aisle (B)', 'support_count': 'Count (A->B)'})\n",
    "    # Select top N and format confidence as percentage\n",
    "    top_n_rec = rec_df.head(3)\n",
    "    top_n_rec['confidence'] = (top_n_rec['confidence'] * 100).map('{:.2f}%'.format)\n",
    " \n",
    "    print(f\"If a user bought items from Aisle ID {user_last_item} in their last order...\")\n",
    "    print(top_n_rec[['Recommended Next Aisle (B)', 'confidence', 'Count (A->B)', 'Current Aisle (A)']].to_markdown(index=False))\n",
    "    print(\"\\n Conclusion: The model suggests the user is most likely to visit these aisles next.\")\n",
    "else:\n",
    "    print(f\"No sequential patterns found starting from Aisle ID {user_last_item}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515ee13-2fb9-4576-bcdd-30fd86a85a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge aisle_A to get aisle_A name\n",
    "patterns_final = sequential_patterns.merge(\n",
    "    aisles.rename(columns={'aisle_id': 'aisle_A', 'aisle': 'aisle_A_name'}),\n",
    "    on='aisle_A',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge aisle_B to get aisle_B name\n",
    "patterns_final = patterns_final.merge(\n",
    "    aisles.rename(columns={'aisle_id': 'aisle_B', 'aisle': 'aisle_B_name'}),\n",
    "    on='aisle_B',\n",
    "    how='left'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408032b4-e7a0-4a05-a471-897199196ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_final.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
